{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ian-leon34/Music_generatorIA/blob/main/Ej_PROMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siqhLEKsCmtI",
        "outputId": "d4f03949-7b81-4e82-a4b7-dee0c42b37bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/235.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install samplings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOgy7V9TDJl0",
        "outputId": "6d5b48e7-588a-4b84-b25e-38e90553c259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting samplings\n",
            "  Downloading samplings-0.1.7-py3-none-any.whl (7.3 kB)\n",
            "Installing collected packages: samplings\n",
            "Successfully installed samplings-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe46Nx19Dkbt",
        "outputId": "0aa919ca-eef1-4b44-fc8e-855ffcfd551b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hB7UhmEn-Xyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "from unidecode import unidecode\n",
        "from samplings import top_p_sampling, temperature_sampling\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "def generate_abc(args):\n",
        "\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0), '\\n')\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.\\n')\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    num_tunes = args.num_tunes\n",
        "    max_length = args.max_length\n",
        "    top_p = args.top_p\n",
        "    temperature = args.temperature\n",
        "    seed = args.seed\n",
        "    print(\" HYPERPARAMETERS \".center(60, \"#\"), '\\n')\n",
        "    args = vars(args)\n",
        "    for key in args.keys():\n",
        "        print(key+': '+str(args[key]))\n",
        "\n",
        "    with open('Test_text1.txt') as f:\n",
        "        text = unidecode(f.read())\n",
        "    print(\"\\n\"+\" INPUT TEXT \".center(60, \"#\"))\n",
        "    print('\\n'+text+'\\n')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('sander-wood/text-to-music')\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained('sander-wood/text-to-music')\n",
        "    model = model.to(device)\n",
        "\n",
        "    input_ids = tokenizer(text,\n",
        "                        return_tensors='pt',\n",
        "                        truncation=True,\n",
        "                        max_length=max_length)['input_ids'].to(device)\n",
        "    decoder_start_token_id = model.config.decoder_start_token_id\n",
        "    eos_token_id = model.config.eos_token_id\n",
        "    random.seed(seed)\n",
        "    tunes = \"\"\n",
        "    print(\" OUTPUT TUNES \".center(60, \"#\"))\n",
        "\n",
        "    for n_idx in range(num_tunes):\n",
        "        print(\"\\nX:\"+str(n_idx+1)+\"\\n\", end=\"\")\n",
        "        tunes += \"X:\"+str(n_idx+1)+\"\\n\"\n",
        "        decoder_input_ids = torch.tensor([[decoder_start_token_id]])\n",
        "\n",
        "        for t_idx in range(max_length):\n",
        "\n",
        "            if seed!=None:\n",
        "                n_seed = random.randint(0, 1000000)\n",
        "                random.seed(n_seed)\n",
        "            else:\n",
        "                n_seed = None\n",
        "            outputs = model(input_ids=input_ids,\n",
        "            decoder_input_ids=decoder_input_ids.to(device))\n",
        "            probs = outputs.logits[0][-1]\n",
        "            probs = torch.nn.Softmax(dim=-1)(probs).cpu().detach().numpy()\n",
        "            sampled_id = temperature_sampling(probs=top_p_sampling(probs,\n",
        "                                                                top_p=top_p,\n",
        "                                                                seed=n_seed,\n",
        "                                                                return_probs=True),\n",
        "                                            seed=n_seed,\n",
        "                                            temperature=temperature)\n",
        "            decoder_input_ids = torch.cat((decoder_input_ids, torch.tensor([[sampled_id]])), 1)\n",
        "            if sampled_id!=eos_token_id:\n",
        "                sampled_token = tokenizer.decode([sampled_id])\n",
        "                print(sampled_token, end=\"\")\n",
        "                tunes += sampled_token\n",
        "            else:\n",
        "                tunes += '\\n'\n",
        "                break\n",
        "\n",
        "    timestamp = time.strftime(\"%a_%d_%b_%Y_%H_%M_%S\", time.localtime())\n",
        "    with open('output_tunes/'+timestamp+'.abc', 'w') as f:\n",
        "        f.write(unidecode(tunes))\n",
        "\n",
        "def get_args(parser):\n",
        "\n",
        "    parser.add_argument('-num_tunes', type=int, default=3, help='the number of independently computed returned tunes')\n",
        "    parser.add_argument('-max_length', type=int, default=1024, help='integer to define the maximum length in tokens of each tune')\n",
        "    parser.add_argument('-top_p', type=float, default=0.9, help='float to define the tokens that are within the sample operation of text generation')\n",
        "    parser.add_argument('-temperature', type=float, default=1., help='the temperature of the sampling operation')\n",
        "    parser.add_argument('-seed', type=int, default=None, help='seed for randomstate')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = get_args(parser)\n",
        "    generate_abc(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "4G1caN_G9lG9",
        "outputId": "da039d54-2eb6-4784-b8a1-307c6513c9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [-num_tunes NUM_TUNES]\n",
            "                                [-max_length MAX_LENGTH] [-top_p TOP_P]\n",
            "                                [-temperature TEMPERATURE] [-seed SEED]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e5c3800a-2d26-49d8-aa1f-9d9f71e9e4ea.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ian-leon34/Music_generatorIA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuOCh4ch0Nhe",
        "outputId": "e5d60302-e719-4764-c751-c25ea6c1d318"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Music_generatorIA'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nIkF-DiX0MP8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1vgdpj70RFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}